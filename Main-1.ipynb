{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7ff5824e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STREAMLINED GNN PIPELINE WITH STATE SAVING\n",
      "============================================================\n",
      "============================================================\n",
      "STEP 1: DATA LOADING AND LABELING\n",
      "============================================================\n",
      " Loaded state from: gnn_pipeline_state\\data_processing_state.pkl\n",
      " Using saved data processing state\n",
      "============================================================\n",
      "STEP 2: PCA WITH STANDARDIZATION\n",
      "============================================================\n",
      " Loaded state from: gnn_pipeline_state\\pca_state.pkl\n",
      " Using saved PCA state\n",
      "============================================================\n",
      "STEP 3: COSINE SIMILARITY COMPUTATION\n",
      "============================================================\n",
      " Loaded state from: gnn_pipeline_state\\similarity_state.pkl\n",
      " Using saved similarity matrix\n",
      "============================================================\n",
      "STEP 4: CONNECTED GRAPH CONSTRUCTION\n",
      "============================================================\n",
      " Loaded state from: gnn_pipeline_state\\threshold_state.pkl\n",
      " Using saved threshold\n",
      " LOADED THRESHOLD: 0.434903\n",
      "============================================================\n",
      "STEP 5: DATA SPLITTING\n",
      "============================================================\n",
      " Loaded state from: gnn_pipeline_state\\split_state.pkl\n",
      "Using saved data splits\n",
      "============================================================\n",
      "STEP 6: TRAINING ALL THREE MODELS\n",
      "============================================================\n",
      "\n",
      "--- Training GCN ---\n",
      " Loaded GCN weights from: gnn_pipeline_state\\GCN_weights.pth\n",
      "Using saved GCN weights\n",
      "GCN Test Accuracy: 0.8286\n",
      "\n",
      "--- Training GAT ---\n",
      " Loaded GAT weights from: gnn_pipeline_state\\GAT_weights.pth\n",
      "Using saved GAT weights\n",
      "GAT Test Accuracy: 0.8571\n",
      "\n",
      "--- Training SAGE ---\n",
      " Loaded SAGE weights from: gnn_pipeline_state\\SAGE_weights.pth\n",
      "Using saved SAGE weights\n",
      "SAGE Test Accuracy: 0.9429\n",
      "\n",
      "============================================================\n",
      "FINAL RESULTS SUMMARY\n",
      "============================================================\n",
      "Graph: 224 nodes, 17326 edges\n",
      "Features: 50 (after PCA)\n",
      "\n",
      "Model Performance:\n",
      "  GCN: 0.8286\n",
      "  GAT: 0.8571\n",
      "  SAGE: 0.9429\n",
      " Saved state to: gnn_pipeline_state\\final_results.pkl\n",
      "\n",
      "Best performing model: SAGE\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import networkx as nx\n",
    "from typing import Dict, Optional\n",
    "import warnings\n",
    "import pickle\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch and PyTorch Geometric imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, GATConv, SAGEConv\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "class StreamlinedGNNPipeline:\n",
    "    \"\"\"\n",
    "    Streamlined GNN Pipeline with essential steps:\n",
    "    1. Data loading and labeling\n",
    "    2. PCA with standardization (50 features)\n",
    "    3. Cosine similarity computation\n",
    "    4. Connected graph construction with minimal edges\n",
    "    5. Training on GCN, GAT, and SAGE models\n",
    "    \n",
    "    NEW: Added state saving/loading functionality for reproducible results\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, file_path: str, save_dir: str = \"gnn_pipeline_state\"):\n",
    "        self.file_path = file_path\n",
    "        self.save_dir = save_dir\n",
    "        self.data = None\n",
    "        self.processed_data = None\n",
    "        self.graph_data = None\n",
    "        self.similarity_matrix = None\n",
    "        self.optimal_threshold = None\n",
    "        \n",
    "        # Create save directory if it doesn't exist\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "    def save_state(self, state_dict: Dict, filename: str):\n",
    "        \"\"\"Save state dictionary to file\"\"\"\n",
    "        filepath = os.path.join(self.save_dir, filename)\n",
    "        with open(filepath, 'wb') as f:\n",
    "            pickle.dump(state_dict, f)\n",
    "        print(f\" Saved state to: {filepath}\")\n",
    "    \n",
    "    def load_state(self, filename: str) -> Optional[Dict]:\n",
    "        \"\"\"Load state dictionary from file\"\"\"\n",
    "        filepath = os.path.join(self.save_dir, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, 'rb') as f:\n",
    "                state_dict = pickle.load(f)\n",
    "            print(f\" Loaded state from: {filepath}\")\n",
    "            return state_dict\n",
    "        return None\n",
    "    \n",
    "    def save_model_state(self, model: nn.Module, model_name: str):\n",
    "        \"\"\"Save model weights\"\"\"\n",
    "        filepath = os.path.join(self.save_dir, f\"{model_name}_weights.pth\")\n",
    "        torch.save(model.state_dict(), filepath)\n",
    "        print(f\" Saved {model_name} weights to: {filepath}\")\n",
    "    \n",
    "    def load_model_state(self, model: nn.Module, model_name: str) -> bool:\n",
    "        \"\"\"Load model weights\"\"\"\n",
    "        filepath = os.path.join(self.save_dir, f\"{model_name}_weights.pth\")\n",
    "        if os.path.exists(filepath):\n",
    "            model.load_state_dict(torch.load(filepath, map_location='cpu'))\n",
    "            print(f\" Loaded {model_name} weights from: {filepath}\")\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "    def load_and_label_data(self) -> Dict:\n",
    "        \"\"\"Step 1: Load data and extract labels\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"STEP 1: DATA LOADING AND LABELING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Check if data processing state exists\n",
    "        saved_state = self.load_state(\"data_processing_state.pkl\")\n",
    "        if saved_state is not None:\n",
    "            print(\" Using saved data processing state\")\n",
    "            return saved_state\n",
    "        \n",
    "        # Load data\n",
    "        self.data = pd.read_csv(self.file_path)\n",
    "        feature_columns = [col for col in self.data.columns \n",
    "                          if col not in ['Energy(keV)', 'Spectrum', 'Acq mode']]\n",
    "        \n",
    "        print(f\"Data loaded: {self.data.shape[0]} samples, {len(feature_columns)} features\")\n",
    "        \n",
    "        # Extract labels\n",
    "        samples_info = []\n",
    "        for idx, row in self.data.iterrows():\n",
    "            sample_name = row['Energy(keV)']\n",
    "            \n",
    "            # Label extraction\n",
    "            if '_3C_' in sample_name or '3C' in sample_name:\n",
    "                label = 0  # Control\n",
    "            elif '_3T_' in sample_name or '3T' in sample_name:\n",
    "                label = 1  # Patient\n",
    "            elif '_5C_' in sample_name or '5C' in sample_name:\n",
    "                label = 0  # Control\n",
    "            elif '_5T_' in sample_name or '5T' in sample_name:\n",
    "                label = 1  # Patient\n",
    "            elif '_1C_' in sample_name or '1C' in sample_name:\n",
    "                label = 0  # Control\n",
    "            elif '_1T_' in sample_name or '1T' in sample_name:\n",
    "                label = 1  # Patient\n",
    "            else:\n",
    "                label = -1  # Unknown\n",
    "            \n",
    "            samples_info.append({\n",
    "                'sample_idx': idx,\n",
    "                'sample_name': sample_name,\n",
    "                'label': label\n",
    "            })\n",
    "        \n",
    "        samples_df = pd.DataFrame(samples_info)\n",
    "        \n",
    "        # Remove unknown labels\n",
    "        valid_mask = samples_df['label'] != -1\n",
    "        samples_df = samples_df[valid_mask].reset_index(drop=True)\n",
    "        valid_data = self.data.loc[samples_df['sample_idx']].reset_index(drop=True)\n",
    "        \n",
    "        # Get features and labels\n",
    "        X = valid_data[feature_columns].values\n",
    "        y = samples_df['label'].values\n",
    "        \n",
    "        print(f\"Valid samples: {len(y)}\")\n",
    "        print(f\"Label distribution - Control: {np.sum(y == 0)}, Patient: {np.sum(y == 1)}\")\n",
    "        \n",
    "        # Save state\n",
    "        state_dict = {\n",
    "            'X': X,\n",
    "            'y': y,\n",
    "            'samples_info': samples_df,\n",
    "            'feature_columns': feature_columns\n",
    "        }\n",
    "        self.save_state(state_dict, \"data_processing_state.pkl\")\n",
    "        \n",
    "        return state_dict\n",
    "    \n",
    "    def apply_pca_standardization(self, X: np.ndarray, n_components: int = 50) -> Dict:\n",
    "        \"\"\"Step 2: Apply PCA with standardization to reduce to 50 features\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"STEP 2: PCA WITH STANDARDIZATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Check if PCA state exists\n",
    "        saved_state = self.load_state(\"pca_state.pkl\")\n",
    "        if saved_state is not None:\n",
    "            print(\" Using saved PCA state\")\n",
    "            return saved_state\n",
    "        \n",
    "        original_dim = X.shape[1]\n",
    "        print(f\"Original features: {original_dim}\")\n",
    "        \n",
    "        # Standardization first\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        print(\"Applied standardization\")\n",
    "        \n",
    "        # PCA with fixed random state\n",
    "        n_components = min(n_components, X.shape[0] - 1, X.shape[1])\n",
    "        pca = PCA(n_components=n_components, random_state=42)\n",
    "        X_pca = pca.fit_transform(X_scaled)\n",
    "        \n",
    "        explained_variance = np.sum(pca.explained_variance_ratio_)\n",
    "        \n",
    "        print(f\"PCA applied: {original_dim} → {X_pca.shape[1]} features\")\n",
    "        print(f\"Explained variance: {explained_variance:.4f} ({100*explained_variance:.2f}%)\")\n",
    "        \n",
    "        # Save state\n",
    "        state_dict = {\n",
    "            'X_pca': X_pca,\n",
    "            'explained_variance': explained_variance,\n",
    "            'scaler': scaler,\n",
    "            'pca': pca\n",
    "        }\n",
    "        self.save_state(state_dict, \"pca_state.pkl\")\n",
    "        \n",
    "        return state_dict\n",
    "    \n",
    "    def compute_cosine_similarity(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Step 3: Compute cosine similarity matrix\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"STEP 3: COSINE SIMILARITY COMPUTATION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Check if similarity state exists\n",
    "        saved_state = self.load_state(\"similarity_state.pkl\")\n",
    "        if saved_state is not None:\n",
    "            print(\" Using saved similarity matrix\")\n",
    "            self.similarity_matrix = saved_state['similarity_matrix']\n",
    "            return self.similarity_matrix\n",
    "        \n",
    "        self.similarity_matrix = cosine_similarity(X)\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Get pairwise similarities (upper triangle, excluding diagonal)\n",
    "        similarities = []\n",
    "        for i in range(n_samples):\n",
    "            for j in range(i + 1, n_samples):\n",
    "                similarities.append(self.similarity_matrix[i, j])\n",
    "        \n",
    "        similarities = np.array(similarities)\n",
    "        \n",
    "        print(f\"Similarity matrix computed: {n_samples} × {n_samples}\")\n",
    "        print(f\"Similarity range: [{np.min(similarities):.4f}, {np.max(similarities):.4f}]\")\n",
    "        print(f\"Mean similarity: {np.mean(similarities):.4f} ± {np.std(similarities):.4f}\")\n",
    "        \n",
    "        # Save state\n",
    "        state_dict = {\n",
    "            'similarity_matrix': self.similarity_matrix\n",
    "        }\n",
    "        self.save_state(state_dict, \"similarity_state.pkl\")\n",
    "        \n",
    "        return self.similarity_matrix\n",
    "    \n",
    "    def find_connected_threshold(self, max_edges: int = 500) -> float:\n",
    "        \"\"\"Step 4: Find threshold for connected graph with minimal edges\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"STEP 4: CONNECTED GRAPH CONSTRUCTION\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Check if threshold state exists\n",
    "        saved_state = self.load_state(\"threshold_state.pkl\")\n",
    "        if saved_state is not None:\n",
    "            print(\" Using saved threshold\")\n",
    "            self.optimal_threshold = saved_state['optimal_threshold']\n",
    "            print(f\" LOADED THRESHOLD: {self.optimal_threshold:.6f}\")\n",
    "            return self.optimal_threshold\n",
    "        \n",
    "        n_samples = self.similarity_matrix.shape[0]\n",
    "        max_possible_edges = n_samples * (n_samples - 1) // 2\n",
    "        \n",
    "        print(f\"Finding connected graph with minimal edges\")\n",
    "        print(f\"Target max edges: {max_edges} (out of {max_possible_edges} possible)\")\n",
    "        \n",
    "        # Get unique similarity values and sort in descending order\n",
    "        similarities = []\n",
    "        for i in range(n_samples):\n",
    "            for j in range(i + 1, n_samples):\n",
    "                similarities.append(self.similarity_matrix[i, j])\n",
    "        \n",
    "        unique_sims = np.unique(similarities)\n",
    "        unique_sims = np.sort(unique_sims)[::-1]  # Descending order\n",
    "        \n",
    "        # Binary search for minimum threshold that ensures connectivity\n",
    "        def is_connected_at_threshold(threshold):\n",
    "            G = nx.Graph()\n",
    "            G.add_nodes_from(range(n_samples))\n",
    "            \n",
    "            edge_count = 0\n",
    "            for i in range(n_samples):\n",
    "                for j in range(i + 1, n_samples):\n",
    "                    if self.similarity_matrix[i, j] >= threshold:\n",
    "                        G.add_edge(i, j)\n",
    "                        edge_count += 1\n",
    "            \n",
    "            return nx.is_connected(G), edge_count\n",
    "        \n",
    "        # Find minimum threshold for connectivity\n",
    "        connected_threshold = None\n",
    "        min_edges_connected = float('inf')\n",
    "        \n",
    "        print(\"Searching for optimal threshold...\")\n",
    "        for i, threshold in enumerate(unique_sims):\n",
    "            is_conn, edge_count = is_connected_at_threshold(threshold)\n",
    "            \n",
    "            if is_conn:\n",
    "                if edge_count <= max_edges:\n",
    "                    # Found connected graph within edge limit\n",
    "                    connected_threshold = threshold\n",
    "                    min_edges_connected = edge_count\n",
    "                    print(f\" Found connected graph: threshold={threshold:.6f}, edges={edge_count}\")\n",
    "                    break\n",
    "                elif connected_threshold is None:\n",
    "                    # First connected graph (may exceed edge limit)\n",
    "                    connected_threshold = threshold\n",
    "                    min_edges_connected = edge_count\n",
    "                    print(f\"⚠️ Connected graph found but exceeds limit: threshold={threshold:.6f}, edges={edge_count}\")\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                status = \"Yes\" if is_conn else \"No\"\n",
    "                print(f\"  Testing threshold {threshold:.6f}: {edge_count} edges, connected={status}\")\n",
    "        \n",
    "        if connected_threshold is None:\n",
    "            # Fallback: use threshold that gives largest component\n",
    "            print(\" No connected graph found, using largest component approach\")\n",
    "            connected_threshold = np.percentile(similarities, 90)\n",
    "        \n",
    "        self.optimal_threshold = connected_threshold\n",
    "        \n",
    "        # Build final graph\n",
    "        final_conn, final_edges = is_connected_at_threshold(connected_threshold)\n",
    "        print(f\"\\n SELECTED THRESHOLD: {connected_threshold:.6f}\")\n",
    "        print(f\"    Edges: {final_edges}\")\n",
    "        print(f\"    Connected: {'YES' if final_conn else 'NO'}\")\n",
    "        print(f\"    Density: {final_edges/max_possible_edges:.6f}\")\n",
    "        \n",
    "        # Save state\n",
    "        state_dict = {\n",
    "            'optimal_threshold': connected_threshold\n",
    "        }\n",
    "        self.save_state(state_dict, \"threshold_state.pkl\")\n",
    "        \n",
    "        return connected_threshold\n",
    "    \n",
    "    def build_graph_data(self, X: np.ndarray, y: np.ndarray) -> Data:\n",
    "        \"\"\"Build PyTorch Geometric graph data\"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # Build edges based on threshold\n",
    "        edge_indices = []\n",
    "        edge_weights = []\n",
    "        \n",
    "        for i in range(n_samples):\n",
    "            for j in range(i + 1, n_samples):\n",
    "                sim = self.similarity_matrix[i, j]\n",
    "                if sim >= self.optimal_threshold:\n",
    "                    edge_indices.append([i, j])\n",
    "                    edge_indices.append([j, i])  # Undirected\n",
    "                    edge_weights.extend([sim, sim])\n",
    "        \n",
    "        if len(edge_indices) > 0:\n",
    "            edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "            edge_attr = torch.tensor(edge_weights, dtype=torch.float)\n",
    "        else:\n",
    "            edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "            edge_attr = torch.empty((0,), dtype=torch.float)\n",
    "        \n",
    "        # Create graph data\n",
    "        self.graph_data = Data(\n",
    "            x=torch.tensor(X, dtype=torch.float),\n",
    "            edge_index=edge_index,\n",
    "            edge_attr=edge_attr,\n",
    "            y=torch.tensor(y, dtype=torch.long)\n",
    "        )\n",
    "        \n",
    "        return self.graph_data\n",
    "    \n",
    "    def split_data(self, train_ratio: float = 0.7, val_ratio: float = 0.15) -> Dict:\n",
    "        \"\"\"Step 5: Split data into train/validation/test sets\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"STEP 5: DATA SPLITTING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Check if split state exists\n",
    "        saved_state = self.load_state(\"split_state.pkl\")\n",
    "        if saved_state is not None:\n",
    "            print(\"Using saved data splits\")\n",
    "            # Apply saved masks to graph data\n",
    "            self.graph_data.train_mask = torch.tensor(saved_state['train_mask'], dtype=torch.bool)\n",
    "            self.graph_data.val_mask = torch.tensor(saved_state['val_mask'], dtype=torch.bool)\n",
    "            self.graph_data.test_mask = torch.tensor(saved_state['test_mask'], dtype=torch.bool)\n",
    "            return saved_state['splits']\n",
    "        \n",
    "        n_nodes = self.graph_data.x.shape[0]\n",
    "        y = self.graph_data.y.numpy()\n",
    "        \n",
    "        # Stratified split with fixed random state\n",
    "        indices = np.arange(n_nodes)\n",
    "        train_idx, temp_idx = train_test_split(\n",
    "            indices, test_size=(1-train_ratio), stratify=y, random_state=42\n",
    "        )\n",
    "        \n",
    "        val_test_ratio = val_ratio / (1-train_ratio)\n",
    "        val_idx, test_idx = train_test_split(\n",
    "            temp_idx, test_size=(1-val_test_ratio), stratify=y[temp_idx], random_state=42\n",
    "        )\n",
    "        \n",
    "        # Create masks\n",
    "        train_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        val_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        test_mask = torch.zeros(n_nodes, dtype=torch.bool)\n",
    "        \n",
    "        train_mask[train_idx] = True\n",
    "        val_mask[val_idx] = True\n",
    "        test_mask[test_idx] = True\n",
    "        \n",
    "        self.graph_data.train_mask = train_mask\n",
    "        self.graph_data.val_mask = val_mask\n",
    "        self.graph_data.test_mask = test_mask\n",
    "        \n",
    "        splits = {'train': train_idx, 'val': val_idx, 'test': test_idx}\n",
    "        \n",
    "        print(f\"Train: {len(train_idx)}, Val: {len(val_idx)}, Test: {len(test_idx)}\")\n",
    "        \n",
    "        # Save state\n",
    "        state_dict = {\n",
    "            'splits': splits,\n",
    "            'train_mask': train_mask.numpy(),\n",
    "            'val_mask': val_mask.numpy(),\n",
    "            'test_mask': test_mask.numpy()\n",
    "        }\n",
    "        self.save_state(state_dict, \"split_state.pkl\")\n",
    "        \n",
    "        return splits\n",
    "    \n",
    "    def train_model(self, model: nn.Module, model_name: str, num_epochs: int = 200, lr: float = 0.01) -> Dict:\n",
    "        \"\"\"Train a GNN model with state saving\"\"\"\n",
    "        # Check if trained model exists\n",
    "        if self.load_model_state(model, model_name):\n",
    "            print(f\"Using saved {model_name} weights\")\n",
    "            # Still return dummy history for consistency\n",
    "            return {'train_acc': [], 'val_acc': []}\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        data = self.graph_data.to(device)\n",
    "        \n",
    "        # Set seeds for reproducibility\n",
    "        torch.manual_seed(42)\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.manual_seed(42)\n",
    "        \n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=5e-4)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        history = {'train_acc': [], 'val_acc': []}\n",
    "        best_val_acc = 0\n",
    "        \n",
    "        print(f\"Training {model.__class__.__name__}...\")\n",
    "        \n",
    "        for epoch in range(num_epochs):\n",
    "            # Training\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data.x, data.edge_index)\n",
    "            loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Evaluation\n",
    "            if epoch % 20 == 0:\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    out = model(data.x, data.edge_index)\n",
    "                    \n",
    "                    # Training accuracy\n",
    "                    train_pred = out[data.train_mask].argmax(dim=1)\n",
    "                    train_acc = (train_pred == data.y[data.train_mask]).float().mean()\n",
    "                    \n",
    "                    # Validation accuracy\n",
    "                    val_pred = out[data.val_mask].argmax(dim=1)\n",
    "                    val_acc = (val_pred == data.y[data.val_mask]).float().mean()\n",
    "                    \n",
    "                    history['train_acc'].append(train_acc.item())\n",
    "                    history['val_acc'].append(val_acc.item())\n",
    "                    \n",
    "                    if val_acc > best_val_acc:\n",
    "                        best_val_acc = val_acc\n",
    "                    \n",
    "                    if epoch % 40 == 0:\n",
    "                        print(f'  Epoch {epoch:3d}: Train Acc: {train_acc:.4f}, Val Acc: {val_acc:.4f}')\n",
    "        \n",
    "        # Save trained model\n",
    "        self.save_model_state(model, model_name)\n",
    "        \n",
    "        return history\n",
    "    \n",
    "    def evaluate_model(self, model: nn.Module) -> Dict:\n",
    "        \"\"\"Evaluate model on test set\"\"\"\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model = model.to(device)\n",
    "        data = self.graph_data.to(device)\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            out = model(data.x, data.edge_index)\n",
    "            test_pred = out[data.test_mask].argmax(dim=1).cpu().numpy()\n",
    "            test_true = data.y[data.test_mask].cpu().numpy()\n",
    "            \n",
    "            test_acc = accuracy_score(test_true, test_pred)\n",
    "            \n",
    "            return {\n",
    "                'test_accuracy': test_acc,\n",
    "                'predictions': test_pred,\n",
    "                'true_labels': test_true,\n",
    "                'classification_report': classification_report(test_true, test_pred, \n",
    "                                                             target_names=['Control', 'Patient'], \n",
    "                                                             output_dict=True, zero_division=0)\n",
    "            }\n",
    "    \n",
    "    def clear_saved_state(self):\n",
    "        \"\"\"Clear all saved states to start fresh\"\"\"\n",
    "        import shutil\n",
    "        if os.path.exists(self.save_dir):\n",
    "            shutil.rmtree(self.save_dir)\n",
    "            os.makedirs(self.save_dir, exist_ok=True)\n",
    "            print(f\" Cleared all saved states from {self.save_dir}\")\n",
    "    \n",
    "    def run_complete_pipeline(self, use_saved_state: bool = True) -> Dict:\n",
    "        \"\"\"Step 6: Run complete pipeline and train all three models\"\"\"\n",
    "        print(\"STREAMLINED GNN PIPELINE WITH STATE SAVING\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if not use_saved_state:\n",
    "            print(\" Starting fresh - clearing saved states\")\n",
    "            self.clear_saved_state()\n",
    "        \n",
    "        # Step 1: Load and label data\n",
    "        data_dict = self.load_and_label_data()\n",
    "        \n",
    "        # Step 2: PCA with standardization\n",
    "        pca_results = self.apply_pca_standardization(data_dict['X'], n_components=50)\n",
    "        \n",
    "        # Step 3: Cosine similarity\n",
    "        self.compute_cosine_similarity(pca_results['X_pca'])\n",
    "        \n",
    "        # Step 4: Find connected threshold\n",
    "        self.find_connected_threshold(max_edges=500)\n",
    "        \n",
    "        # Build graph\n",
    "        self.build_graph_data(pca_results['X_pca'], data_dict['y'])\n",
    "        \n",
    "        # Step 5: Split data\n",
    "        splits = self.split_data()\n",
    "        \n",
    "        # Step 6: Train all three models\n",
    "        print(\"=\"*60)\n",
    "        print(\"STEP 6: TRAINING ALL THREE MODELS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        input_dim = self.graph_data.x.shape[1]\n",
    "        hidden_dim = 64\n",
    "        num_classes = 2\n",
    "        \n",
    "        models = {\n",
    "            'GCN': GCNModel(input_dim, hidden_dim, num_classes),\n",
    "            'GAT': GATModel(input_dim, hidden_dim, num_classes),\n",
    "            'SAGE': SAGEModel(input_dim, hidden_dim, num_classes)\n",
    "        }\n",
    "        \n",
    "        results = {}\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"\\n--- Training {model_name} ---\")\n",
    "            history = self.train_model(model, model_name, num_epochs=200)\n",
    "            evaluation = self.evaluate_model(model)\n",
    "            \n",
    "            print(f\"{model_name} Test Accuracy: {evaluation['test_accuracy']:.4f}\")\n",
    "            \n",
    "            results[model_name] = {\n",
    "                'model': model,\n",
    "                'history': history,\n",
    "                'evaluation': evaluation\n",
    "            }\n",
    "        \n",
    "        # Summary\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"FINAL RESULTS SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Graph: {self.graph_data.x.shape[0]} nodes, {self.graph_data.edge_index.shape[1]//2} edges\")\n",
    "        print(f\"Features: {input_dim} (after PCA)\")\n",
    "        print(\"\\nModel Performance:\")\n",
    "        for model_name, result in results.items():\n",
    "            acc = result['evaluation']['test_accuracy']\n",
    "            print(f\"  {model_name}: {acc:.4f}\")\n",
    "        \n",
    "        # Save final results\n",
    "        final_results = {\n",
    "            'model_performances': {name: result['evaluation']['test_accuracy'] \n",
    "                                 for name, result in results.items()},\n",
    "            'graph_info': {\n",
    "                'nodes': self.graph_data.x.shape[0],\n",
    "                'edges': self.graph_data.edge_index.shape[1]//2,\n",
    "                'features': input_dim\n",
    "            }\n",
    "        }\n",
    "        self.save_state(final_results, \"final_results.pkl\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "\n",
    "# GNN Model Definitions (unchanged)\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.5):\n",
    "        super(GCNModel, self).__init__()\n",
    "        self.conv1 = GCNConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GCNConv(hidden_dim, num_classes)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class GATModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.5, heads=8):\n",
    "        super(GATModel, self).__init__()\n",
    "        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_dim * heads, num_classes, heads=1, dropout=dropout)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "class SAGEModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_classes, dropout=0.5):\n",
    "        super(SAGEModel, self).__init__()\n",
    "        self.conv1 = SAGEConv(input_dim, hidden_dim)\n",
    "        self.conv2 = SAGEConv(hidden_dim, num_classes)\n",
    "        self.dropout = dropout\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize pipeline\n",
    "    pipeline = StreamlinedGNNPipeline('flou.csv')\n",
    "    \n",
    "    # Run complete pipeline (uses saved state by default)\n",
    "    # First run: use_saved_state=False to compute and save everything\n",
    "    # Subsequent runs: use_saved_state=True (default) to load saved state\n",
    "    results = pipeline.run_complete_pipeline(use_saved_state=True)\n",
    "    \n",
    "    # Access individual model results\n",
    "    gcn_accuracy = results['GCN']['evaluation']['test_accuracy']\n",
    "    gat_accuracy = results['GAT']['evaluation']['test_accuracy']\n",
    "    sage_accuracy = results['SAGE']['evaluation']['test_accuracy']\n",
    "    \n",
    "    print(f\"\\nBest performing model: {max(results.keys(), key=lambda x: results[x]['evaluation']['test_accuracy'])}\")\n",
    "    \n",
    "    # To start fresh and recompute everything:\n",
    "    # results = pipeline.run_complete_pipeline(use_saved_state=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
